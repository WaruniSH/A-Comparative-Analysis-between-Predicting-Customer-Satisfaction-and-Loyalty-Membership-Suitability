{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "dataset=pd.read_csv('/content/restaurant_customer_satisfaction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View Dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing first five rows of the dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing last three raws of the dataset\n",
    "dataset.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing data raws upto zero to twenty 3 by 3\n",
    "dataset.loc[0:20:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing the dimention of the dataset\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the coloumns names of the dataset\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing dataset as a dataframe\n",
    "df=pd.DataFrame(dataset)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the detailed informations of the features\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing specific colomn only\n",
    "df['Income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing specific rows of a specific colomn\n",
    "df.loc[0:1,'Income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Age\", \"Gender\", \"MealType\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the mean of FoodRating colomn\n",
    "meann = df['FoodRating'].mean()\n",
    "meann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round the mean in to two points\n",
    "round(meann,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the data types of the colomns\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the total number of null values in the data set according to the features\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unwanted columns\n",
    "dataset = dataset.drop(['CustomerID','Gender', 'GroupSize', 'MealType', 'OnlineReservation', 'DeliveryOrder', 'WaitTime', 'AverageSpend'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the dataset after dropping the unwanted colomns\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the statistical description of the data(only Numarical data will describe)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, PowerTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a copy of dataset\n",
    "original_dataset = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#differntiate the data(numarical/catergorical/bianary/non-binary)\n",
    "non_binary_categorical_columns =['VisitFrequency', 'PreferredCuisine', 'TimeOfVisit', 'DiningOccasion']\n",
    "numarical_columns = ['Age', 'Income',  'LoyaltyProgramMember', 'ServiceRating', 'FoodRating', 'AmbianceRating','HighSatisfaction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply one-hot encoding\n",
    "VisitFrequency_dummie = pd.get_dummies(original_dataset['VisitFrequency'], prefix = 'VisitFrequency')\n",
    "PreferredCuisine_dummie = pd.get_dummies(original_dataset['PreferredCuisine'], prefix = 'PreferredCuisine')\n",
    "TimeOfVisit_dummie = pd.get_dummies(original_dataset['TimeOfVisit'], prefix = 'TimeOfVisit')\n",
    "DiningOccasion_dummie = pd.get_dummies(original_dataset['DiningOccasion'], prefix = 'DiningOccasion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the non-binary categorical colomns from the copy of dataset and stote in another variable\n",
    "new_data = original_dataset.drop(non_binary_categorical_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate the dummies\n",
    "OH_data = pd.concat([new_data,VisitFrequency_dummie,PreferredCuisine_dummie,TimeOfVisit_dummie,DiningOccasion_dummie], axis=1)\n",
    "print (OH_data)\n",
    "\n",
    "\n",
    "OH_data.columns = OH_data.columns.astype(str)\n",
    "\n",
    "OH_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a copy of encoded data and check the null values\n",
    "encoded_data = OH_data.copy()\n",
    "encoded_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions for numerical features\n",
    "numeric_columns = ['Income', 'HighSatisfaction',  'LoyaltyProgramMember', 'ServiceRating', 'FoodRating', 'AmbianceRating']\n",
    "for column in numeric_columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df[column].dropna(), kde=True)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.displot(encoded_data[\"Income\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(encoded_data[\"HighSatisfaction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(encoded_data[\"LoyaltyProgramMember\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the boxplots of some features\n",
    "plt.boxplot(encoded_data['ServiceRating'])\n",
    "plt.title('Service Rating Explot')\n",
    "plt.ylabel('Service Rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(encoded_data['FoodRating'])\n",
    "plt.title('Food Rating Explot')\n",
    "plt.ylabel('Food Rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(encoded_data['AmbianceRating'])\n",
    "plt.title('Ambiance Rating Explot')\n",
    "plt.ylabel('AmbianceRating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the first five rowas of the encoded data and se  the features\n",
    "encoded_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the target variable from the encoded data and store in another variable\n",
    "independant_features = encoded_data.columns.drop(['LoyaltyProgramMember']).tolist()\n",
    "\n",
    "#define the X and Y for split the dataset\n",
    "y = encoded_data['LoyaltyProgramMember']\n",
    "X = encoded_data[independant_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardization\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(train_X)\n",
    "# X_test_scaled = scaler.transform(val_X)\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler().set_output(transform=\"pandas\")\n",
    "X_normalized = scaler.fit_transform(X) # Noralized X\n",
    "\n",
    "X_normalized.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset\n",
    "train_X, val_X, train_y, val_y = train_test_split(X_normalized, y, random_state = 10 ,test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create instance of algorithms\n",
    "lr = LogisticRegression()\n",
    "knn = KNeighborsClassifier()\n",
    "svc = SVC()\n",
    "rf = RandomForestClassifier()\n",
    "gb = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the training data for algorithms\n",
    "lr.fit(train_X,train_y)\n",
    "knn.fit(train_X,train_y)\n",
    "svc.fit(train_X,train_y)\n",
    "rf.fit(train_X,train_y)\n",
    "gb.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the predictions using validation or the testing data\n",
    "y_pred1 = lr.predict(val_X)\n",
    "y_pred2 = knn.predict(val_X)\n",
    "y_pred3 = svc.predict(val_X)\n",
    "y_pred4 = rf.predict(val_X)\n",
    "y_pred5 = gb.predict(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the accuracy of each algorithms by using predicted values\n",
    "print('Logistic Regression Accuracy:',accuracy_score(val_y,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('KNN Accuracy:',accuracy_score(val_y,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVC Accuracy:',accuracy_score(val_y,y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Forest Accuracy:',accuracy_score(val_y,y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient Boosting Accuracy:',accuracy_score(val_y,y_pred5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the models\n",
    "pickle.dump(lr,open('model.pkl','wb'))\n",
    "pickle.dump(svc,open('scaler.pkl','wb'))\n",
    "pickle.dump(knn,open('knn.pkl','wb'))\n",
    "pickle.dump(rf,open('rf.pkl','wb'))\n",
    "pickle.dump(gb,open('gb.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'gradient_bossting.sav'\n",
    "\n",
    "# Save the model\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(gb, file)\n",
    "print(f\"Model saved as {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparametertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For machine learning models\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# For hyperparameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "# For performance metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# SVC hyperparameter grid\n",
    "param_grid_svc = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['linear', 'rbf', 'poly']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier tuning\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "random_search_rf = RandomizedSearchCV(estimator=rf_model,\n",
    "                                      param_distributions=param_grid_rf,\n",
    "                                      n_iter=100,\n",
    "                                      cv=3,\n",
    "                                      verbose=2,\n",
    "                                      random_state=42,\n",
    "                                      n_jobs=-1)\n",
    "random_search_rf.fit(train_X, train_y)\n",
    "\n",
    "# GradientBoostingClassifier tuning\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "random_search_gb = RandomizedSearchCV(estimator=gb_model,\n",
    "                                      param_distributions=param_grid_gb,\n",
    "                                      n_iter=100,\n",
    "                                      cv=3,\n",
    "                                      verbose=2,\n",
    "                                      random_state=42,\n",
    "                                      n_jobs=-1)\n",
    "random_search_gb.fit(train_X, train_y)\n",
    "\n",
    "# SVC tuning using GridSearchCV (since the grid is smaller)\n",
    "svc_model = SVC()\n",
    "grid_search_svc = GridSearchCV(estimator=svc_model,\n",
    "                               param_grid=param_grid_svc,\n",
    "                               cv=3,\n",
    "                               verbose=2,\n",
    "                               n_jobs=-1)\n",
    "grid_search_svc.fit(train_X, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters for RandomForest: \", random_search_rf.best_params_)\n",
    "print(\"Best hyperparameters for GradientBoosting: \", random_search_gb.best_params_)\n",
    "print(\"Best hyperparameters for SVC: \", grid_search_svc.best_params_)\n",
    "\n",
    "# Best cross-validation scores\n",
    "print(\"Best cross-validation score for RandomForest: \", random_search_rf.best_score_)\n",
    "print(\"Best cross-validation score for GradientBoosting: \", random_search_gb.best_score_)\n",
    "print(\"Best cross-validation score for SVC: \", grid_search_svc.best_score_)\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred_rf = random_search_rf.predict(val_X)\n",
    "y_pred_gb = random_search_gb.predict(val_X)\n",
    "y_pred_svc = grid_search_svc.predict(val_X)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"RandomForest Accuracy: \", accuracy_score(val_y, y_pred_rf))\n",
    "print(\"GradientBoosting Accuracy: \", accuracy_score(val_y, y_pred_gb))\n",
    "print(\"SVC Accuracy: \", accuracy_score(val_y, y_pred_svc))\n",
    "\n",
    "print(\"RandomForest Classification Report:\\n\", classification_report(val_y, y_pred_rf))\n",
    "print(\"GradientBoosting Classification Report:\\n\", classification_report(val_y, y_pred_gb))\n",
    "print(\"SVC Classification Report:\\n\", classification_report(val_y, y_pred_svc))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"RandomForest Confusion Matrix:\\n\", confusion_matrix(val_y, y_pred_rf))\n",
    "print(\"GradientBoosting Confusion Matrix:\\n\", confusion_matrix(val_y, y_pred_gb))\n",
    "print(\"SVC Confusion Matrix:\\n\", confusion_matrix(val_y, y_pred_svc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best models\n",
    "best_rf_model = random_search_rf.best_estimator_\n",
    "best_gb_model = random_search_gb.best_estimator_\n",
    "best_svc_model = grid_search_svc.best_estimator_\n",
    "\n",
    "# Feature importance for RandomForest\n",
    "importances_rf = best_rf_model.feature_importances_\n",
    "plt.barh(train_X.columns, importances_rf)  # Assuming X_train is a DataFrame\n",
    "plt.title(\"RandomForest Feature Importances\")\n",
    "plt.show()\n",
    "\n",
    "# Feature importance for GradientBoosting\n",
    "importances_gb = best_gb_model.feature_importances_\n",
    "plt.barh(train_X.columns, importances_gb)  # Assuming X_train is a DataFrame\n",
    "plt.title(\"GradientBoosting Feature Importances\")\n",
    "plt.show()\n",
    "\n",
    "# SVC does not have feature importances, since it's not a tree-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Function to plot multiple confusion matrices in one figure\n",
    "def plot_confusion_matrices(y_true, preds, model_names):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))  # Create 1 row and 3 columns of subplots\n",
    "    for ax, y_pred, model_name in zip(axes, preds, model_names):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)\n",
    "        ax.set_title(f'Confusion Matrix for {model_name}')\n",
    "        ax.set_ylabel('Actual')\n",
    "        ax.set_xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Predictions from the models\n",
    "preds = [y_pred_rf, y_pred_gb, y_pred_svc]\n",
    "model_names = [\"RandomForestClassifier\", \"GradientBoostingClassifier\", \"SVC\"]\n",
    "\n",
    "# Plot the confusion matrices for all models\n",
    "plot_confusion_matrices(val_y, preds, model_names)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
